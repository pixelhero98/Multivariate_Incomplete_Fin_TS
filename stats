def diffusion_loss(model: LLapDiT, scheduler, x0_lat_norm: torch.Tensor, t: torch.Tensor,
                   *, cond_summary: Optional[torch.Tensor], predict_type: str = "v") -> torch.Tensor:
    """
    MSE on v/eps with channel-invariant reduction.
    Optional min-SNR weighting (set in crypto_config).
    """
    noise = torch.randn_like(x0_lat_norm)
    x_t, eps_true = scheduler.q_sample(x0_lat_norm, t, noise)
    pred = model(x_t, t, cond_summary=cond_summary, sc_feat=None)
    target = eps_true if predict_type == "eps" else scheduler.v_from_eps(x_t, t, eps_true)

    # [B,H,Z] -> per-sample loss: mean over H, sum over Z  (scale-invariant to Z)
    err = (pred - target).pow(2)              # [B,H,Z]
    per_sample = err.mean(dim=1).sum(dim=1)   # [B]

    # Optional: min-SNR weighting to prevent early-t domination
    if getattr(crypto_config, "USE_MIN_SNR", False):
        abar = scheduler.alpha_bars[t]                                   # [B]
        snr  = abar / (1.0 - abar).clamp_min(1e-8)
        gamma = float(getattr(crypto_config, "MIN_SNR_GAMMA", 5.0))
        w = torch.minimum(snr, torch.as_tensor(gamma, device=snr.device, dtype=snr.dtype))
        w = w / (snr + 1.0)                                             # keeps magnitude reasonable
        return (w.detach() * per_sample).mean()
    else:
        return per_sample.mean()


Z = crypto_config.VAE_LATENT_DIM
print(f"Epoch {epoch:03d} | train: {train_loss:.6f} (/Z: {train_loss/Z:.6f}) "
      f"| val: {val_loss:.6f} (/Z: {val_loss/Z:.6f}) | cond_gap: {cond_gap:.6f}")

per_dim = (pred - target).pow(2).mean(dim=(0,1))  # [Z]
vals, idx = torch.topk(per_dim, k=min(8, per_dim.numel()))
print("val top-8 latent dims:", ", ".join(f"{int(i)}:{v.item():.4f}" for v,i in zip(vals, idx)))


sizes: (149372, 21284, 42861)
V: torch.Size([20, 121, 60, 17]) T: torch.Size([20, 121, 60, 17]) y: torch.Size([20, 121, 20])
Loaded VAE checkpoint: ./ldt/saved_model/16_0.00124_2.16059_best_recon.pt
Epoch 001 | train: 2.800017 (/Z: 0.175001) | val: 2.641022 (/Z: 0.165064) | cond_gap: 0.000413
Saved: ./ldt/checkpoints/best_latdiff_epoch_001_val_2.641022.pt
Epoch 002 | train: 2.796107 (/Z: 0.174757) | val: 2.554339 (/Z: 0.159646) | cond_gap: 0.020972
Saved: ./ldt/checkpoints/best_latdiff_epoch_002_val_2.554339.pt
Epoch 003 | train: 2.766004 (/Z: 0.172875) | val: 2.580292 (/Z: 0.161268) | cond_gap: 0.008501
Epoch 004 | train: 2.711265 (/Z: 0.169454) | val: 2.525406 (/Z: 0.157838) | cond_gap: -0.008521
Saved: ./ldt/checkpoints/best_latdiff_epoch_004_val_2.525406.pt
Epoch 005 | train: 2.664506 (/Z: 0.166532) | val: 2.578474 (/Z: 0.161155) | cond_gap: -0.023356
Epoch 006 | train: 2.629713 (/Z: 0.164357) | val: 2.538120 (/Z: 0.158633) | cond_gap: -0.006521
