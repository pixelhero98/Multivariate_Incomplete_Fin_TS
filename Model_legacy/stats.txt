def diffusion_loss(model: LLapDiT, scheduler, x0_lat_norm: torch.Tensor, t: torch.Tensor,
                   *, cond_summary: Optional[torch.Tensor], predict_type: str = "v") -> torch.Tensor:
    """
    MSE on v/eps with channel-invariant reduction.
    Optional min-SNR weighting (set in crypto_config).
    """
    noise = torch.randn_like(x0_lat_norm)
    x_t, eps_true = scheduler.q_sample(x0_lat_norm, t, noise)
    pred = model(x_t, t, cond_summary=cond_summary, sc_feat=None)
    target = eps_true if predict_type == "eps" else scheduler.v_from_eps(x_t, t, eps_true)

    # [B,H,Z] -> per-sample loss: mean over H, sum over Z  (scale-invariant to Z)
    err = (pred - target).pow(2)              # [B,H,Z]
    per_sample = err.mean(dim=1).sum(dim=1)   # [B]

    # Optional: min-SNR weighting to prevent early-t domination
    if getattr(crypto_config, "USE_MIN_SNR", False):
        abar = scheduler.alpha_bars[t]                                   # [B]
        snr  = abar / (1.0 - abar).clamp_min(1e-8)
        gamma = float(getattr(crypto_config, "MIN_SNR_GAMMA", 5.0))
        w = torch.minimum(snr, torch.as_tensor(gamma, device=snr.device, dtype=snr.dtype))
        w = w / (snr + 1.0)                                             # keeps magnitude reasonable
        w = w / (w.mean().detach() + 1e-8)
        return (w.detach() * per_sample).mean()
    else:
        return per_sample.mean()


Z = crypto_config.VAE_LATENT_DIM
print(f"Epoch {epoch:03d} | train: {train_loss:.6f} (/Z: {train_loss/Z:.6f}) "
      f"| val: {val_loss:.6f} (/Z: {val_loss/Z:.6f}) | cond_gap: {cond_gap:.6f}")

per_dim = (pred - target).pow(2).mean(dim=(0,1))  # [Z]
vals, idx = torch.topk(per_dim, k=min(8, per_dim.numel()))
print("val top-8 latent dims:", ", ".join(f"{int(i)}:{v.item():.4f}" for v,i in zip(vals, idx)))

@torch.no_grad()
def validate():
    diff_model.eval()
    total, count = 0.0, 0
    cond_gap_accum, cond_gap_batches = 0.0, 0

    # add this guard so we only print once per epoch
    did_diag_per_dim = False

    if ema is not None:
        ema.store(diff_model)
        ema.copy_to(diff_model)

    for xb, yb, meta in val_dl:
        V, T = xb
        mask_bn = meta["entity_mask"]

        cond_summary = build_context(diff_model, V, T, mask_bn, device)
        y_in, batch_ids = flatten_targets(yb, mask_bn, device)
        if y_in is None:
            continue
        cond_summary_flat = cond_summary[batch_ids]
        mu_norm = encode_mu_norm(
            vae, y_in,
            use_ewma=crypto_config.USE_EWMA,
            ewma_lambda=crypto_config.EWMA_LAMBDA,
            mu_mean=mu_mean, mu_std=mu_std
        )

        t = sample_t_uniform(scheduler, mu_norm.size(0), device)
        loss = diffusion_loss(
            diff_model, scheduler, mu_norm, t,
            cond_summary=cond_summary_flat, predict_type=crypto_config.PREDICT_TYPE
        )
        total += loss.item() * mu_norm.size(0)
        count += mu_norm.size(0)

        # ---------- Conditional gap probe (existing) ----------
        probe_n = min(128, mu_norm.size(0))
        if probe_n > 0:
            mu_p = mu_norm[:probe_n]
            cs_p = cond_summary_flat[:probe_n]
            t_p  = sample_t_uniform(scheduler, probe_n, device)
            loss_cond = diffusion_loss(diff_model, scheduler, mu_p, t_p,
                                       cond_summary=cs_p, predict_type=crypto_config.PREDICT_TYPE).item()
            loss_unco = diffusion_loss(diff_model, scheduler, mu_p, t_p,
                                       cond_summary=None, predict_type=crypto_config.PREDICT_TYPE).item()
            cond_gap_accum += (loss_unco - loss_cond)
            cond_gap_batches += 1

        # ---------- NEW: per-dimension loss probe ----------
        if getattr(crypto_config, "DIAG_PER_DIM", False) and not did_diag_per_dim:
            # small sub-batch to keep it cheap
            Bp = min(64, mu_norm.size(0))
            mu_p = mu_norm[:Bp]
            cs_p = cond_summary_flat[:Bp]
            t_p  = sample_t_uniform(scheduler, Bp, device)

            # recreate pred & target to get per-dim errors
            noise_p = torch.randn_like(mu_p)
            x_t_p, eps_true_p = scheduler.q_sample(mu_p, t_p, noise_p)

            pred_p = diff_model(x_t_p, t_p, cond_summary=cs_p, sc_feat=None)
            if crypto_config.PREDICT_TYPE == "eps":
                target_p = eps_true_p
            else:  # "v"
                target_p = scheduler.v_from_eps(x_t_p, t_p, eps_true_p)

            # pred/target shape: [B, H, Z]; mean over B & H -> per-dim [Z]
            per_dim = (pred_p - target_p).pow(2).mean(dim=(0, 1))  # [Z]
            k = min(8, per_dim.numel())
            vals, idx = torch.topk(per_dim, k=k)
            print("val top-{} latent dims: {}".format(
                k, ", ".join(f"{int(i)}:{v.item():.4f}" for v, i in zip(vals, idx))
            ))
            # also useful: report mean and std of per-dim losses
            print(f"per-dim loss mean={per_dim.mean().item():.4f} std={per_dim.std(unbiased=False).item():.4f}")

            did_diag_per_dim = True  # only once per epoch

    if ema is not None:
        ema.restore(diff_model)

    avg_val = total / max(1, count)
    cond_gap = (cond_gap_accum / cond_gap_batches) if cond_gap_batches > 0 else float("nan")
    return avg_val, cond_gap
